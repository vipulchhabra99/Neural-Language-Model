# -*- coding: utf-8 -*-
"""Copy_of_Copy_of_Copy_of_Copy_of_LanguageModel (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rewyH2prWhqr58Xm_E0rGIHNpEASCkjC

**For Downloading The Dataset**
"""

import os
import urllib.request

urllib.request.urlretrieve('https://drive.google.com/u/0/uc?id=1L8WgYskOPXvnU1ZmbEnIX12Po8CBiVw-&export=download', 'dataset.txt')

"""**Cleaning and Preparation of the Dataset**



"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
import string
import re
from sklearn.model_selection import train_test_split
import random
from collections import Counter

dataset = open("dataset.txt", "r")
dataset = dataset.read()

def datapreprocessing(dataset):
  paragraphs = re.split('#*#', dataset)
  sentences = []

  for para in paragraphs:
    sentences.append(para.split("."))

  sentences = [('<START> ' + sentence + ' <END>') for para in sentences for sentence in para if len(sentence) > 0]
  sentences = list(set(sentences))
  sentences[:10]
  # random.shuffle(sentences)
  train_size = (len(sentences) * 7) // 10
  test_size = (len(sentences) * 2) // 10
  train = sentences[:train_size]
  test = sentences[train_size:train_size + test_size]
  validation = sentences[train_size + test_size:]
  
  return train, test, validation, sentences

def datacleaning(sentences):

  if(type(sentences) == list):
    sentences = " ".join(sentences)
  tokens = sentences.split()
  tokens = [word if word == '<START>' or word == '<END>' else word.lower() for word in tokens]
  table = str.maketrans('', '', string.punctuation)
  tokens = [word if word == '<START>' or word == '<END>' else word.translate(table) for word in tokens]
  tokens = [word for word in tokens if word.isalpha() or word == '<START>' or word == '<END>']
  return tokens

train, test, validate, full_dataset = datapreprocessing(dataset)
cleaned_train = datacleaning(train)
cleaned_test = datacleaning(test)
cleaned_validate = datacleaning(validate)
cleaned_fulldataset = datacleaning(full_dataset)

max_length = 200
trunc_type = 'post'
padding_type = 'post'
oov_tok = '<OOV>'
vocab_size = 500

def prepare_ngram(sentences, n_gram):
  ngrams = []
  for i in range(len(sentences)-n_gram):
    seq = sentences[i : i + n_gram]
    ngrams.append(seq)
  return ngrams

"""**Neural Model**"""

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding, RNN, GRU, Bidirectional
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(cleaned_fulldataset)
word_index = tokenizer.word_index

train_sequences = tokenizer.texts_to_sequences(cleaned_train)
validate_sequences = tokenizer.texts_to_sequences(cleaned_validate)

train_sequences = [token for sublist in train_sequences for token in sublist]
validate_sequences = [token for sublist in validate_sequences for token in sublist]

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

def prepare_train_dataset(dataset):
  X = []
  Y = []
  for ngram in range(2, 6):
    res_ngram = prepare_ngram(dataset, ngram)
    for ngram in res_ngram:
      X.append(ngram[:-1])
      Y.append(ngram[-1])

  X = pad_sequences(X, maxlen = 4, value = 0,padding='post')
  Y_or = to_categorical(Y, vocab_size)

  return X, Y_or, Y
X_train, Y_train, _ = prepare_train_dataset(train_sequences)

X_valid, Y_valid, _ = prepare_train_dataset(validate_sequences)


seq_length = X_train.shape[1]

reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

from keras import optimizers

model = Sequential()
model.add(Embedding(vocab_size, 96,input_length = seq_length))
model.add(Bidirectional(LSTM(100, dropout = 0.2)))
model.add(Dense(vocab_size, activation='softmax'))
sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.8, nesterov=True)

model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

num_epochs = 10
model.fit(X_train, Y_train, epochs=num_epochs, validation_data=(X_valid, Y_valid), batch_size = 32)

import math
def get_perplexity(in_string, model):
    in_tokens = datacleaning(in_string)
    in_ids = tokenizer.texts_to_sequences(in_tokens)
    X, _, Y = prepare_train_dataset(in_ids)
    if(len(X) == 0):
        return "NOT"

    preds = model.predict(X)
    log_prob = 0.0
    for y_i, y in enumerate(Y):
        log_prob += np.log(preds[y_i, y])

    log_prob = log_prob/len(Y)
    return math.exp(log_prob)

train_perplexity = []
f = open('2019121001-LM1-train-perplexity.txt', 'w')
for in_string in train:
    log_prob = get_perplexity(in_string, model)
    if(str(log_prob) != "NOT"):
      log_prob = 1/log_prob
      train_perplexity.append(log_prob)
      result_string = in_string + "\t" + str(log_prob) + "\n"
      f.write(result_string)

f.write("Average perplexity score for the whole dataset :" + "\t" + str(np.mean(train_perplexity))+"\n")
f.close()

validate_perplexity = []
for in_string in validate:
    log_prob = get_perplexity(in_string, model)
    if(str(log_prob) != "NOT"):
      log_prob = 1/log_prob
      validate_perplexity.append(log_prob)

f = open('2019121001-LM1-test-perplexity.txt', 'w')
test_perplexity = []
for in_string in test:
    log_prob = get_perplexity(in_string, model)
    if(str(log_prob) != "NOT"):
      log_prob = 1/log_prob
      test_perplexity.append(log_prob)
      result_string = in_string + "\t" + str(log_prob) + "\n"
      f.write(result_string)

f.write("Average perplexity score for the whole dataset :" + "\t" + str(np.mean(test_perplexity))+"\n")
f.close()

train_perplexity = np.array(train_perplexity)
validate_perplexity = np.array(validate_perplexity)
test_perplexity = np.array(test_perplexity)

print("Average Train Perplexity Score :", np.mean(train_perplexity))
print("Average Validate Perplexity Score :", np.mean(validate_perplexity))
print("Average Test Perplexity Score :", np.mean(test_perplexity))

print("Ready !")

x = input()
while(x != "quit()"):
  new_line = '<START> ' + x + ' <END>'
  log_prob = get_perplexity(new_line, model)
  if(str(log_prob) != "NOT"):
      log_prob = 1/log_prob
      print("Perplexity Score : ", log_prob)

  else:
    print("Please enter the valid string !")
  x = input()
  if(x == "quit()"):
    break
